\chapter{Evaluation}

\begin{itemize}
\item show if and how objectives mentioned in the introduction are met by the implementation
\item use ilustrating examples
\end{itemize}

\section{Optimization}
Standard deviation
Average mean

\section{Comparison}
While there are multiple approaches to parallel programming in C, to the knowledge of the writer none offers the combination of thread-safety by design and approaches for optimization. Most progress seems to come from other languages. Therefore, the following paragraphs will presented relevant ideas of a few languages that either already influenced the design and optimization of ParallelMbeddr or might do so in the future. The comparison starts with a short depiction of one of the most well-known parallelization enhancements\footnote{Enhancement is meant in the sense of easier utilization.} for C.

\subsection{OpenMP}
\textit{OpenMP}\footnote{http://openmp.org/wp/, accessed: 2014-08-23} is an API to parallel programming for C/C++ and Fortran\cite[p.~1]{OpenMP}. It supports both task parallelism and data parallelism. OpenMP is used by extending valid C code with the \CODE{\#pragma} directive\footnote{See https://gcc.gnu.org/onlinedocs/cpp/Pragmas.html for details.}, which instructs the compiler how to parallelize a program. It can therefore be seen as a declarative way of enriching single-threaded programs with parallelism. OpenMP it does not check the code for correctness regarding thread-safety. This means that it is the programmer's responsibility to ensure that data dependencies do not entail data races. Nevertheless, it offers extensive support for parallelization. For instance, OpenMP offers multiple strategies to distribute the processing of arrays onto multiple threads. This, of course, is much easier to realize with the absence of strong thread-safety guarantees. The following language, on the other, hand fills this gap by combining both features. In this regard it might serve as one possible Paragon for the future development of ParallelMbeddr.

\subsection{ParaSail}
The programming language \textit{ParaSail} offers implicit parallelism. It does so by guaranteeing at compile time that theoretically every valid expression can be evaluated in parallel. In contrast to `pure' languages like Haskell, it does not sacrifice side-effects for this purpose.\footnote{http://parasail-programming-language.blogspot.de/2012/08/a-pointer-free-path-to-object-oriented.html, accessed: 2014-08-23} However, in order to automatically prove the safeness of parallel executions, it avoids concepts that are crucial to languages like C, like pointers and global variables.\footnote{http://www.embedded.com/design/programming-languages-and-tools/4375616/1/ParaSail--Less-is-more-with-multicore, accessed: 2014-08-23} In these regards it differs from the focus of this work. Nevertheless, its approach to data collections may also provide ideas for future research: ParaSail makes heavy use of indexable containers like lists or trees. When equipped with proper pre- and postconditions, containers can be sliced in a thread-safe manner \cite{ParaSailReferenceManual}. This means that they must be designed in a way which lets the compiler prove that slices over their elements do not overlap. In such a case segmented views of the original container can be manipulated in a thread-safe manner in parallel. While this feature seems to be promising for future enhancements of ParallelMbeddr, it should be noted that the presence of pointers in C could impede an according realization of containers.

\subsection{\AE minium}
Like ParaSail, the programming language \textit{\AE minium} realizes implicit parallelism. As opposed to ParaSail, it uses explicit annotations on methods by the user to guarantee thread-safety. These annotations declare \textit{access permissions} (\cite{ModularTypestateChecking}) on the parameters of function arguments and return values. The compiler uses these access permissions (\textit{shared}, \textit{immutable}, \textit{unique}) in order to prove the correctness of programs in terms of thread-safety. If possible, executions are parallelized automatically. This may for instance happen if only immutable data is involved. On the contrary, the presence of mutable shared data requires the programmer to wrap according statements inside atomic blocks. While atomic blocks could be inferred by the compiler, the language pursues this path ``for granularity reasons'' \cite{ConcurrencyByDefault}. This approach equals ParallelMbeddr's requirement to annotate synchronization blocks explicitly. Both languages thus enable the user to ``have fine-grain control over the size of critical sections''. Thus, the user controls the trade-off between the responsiveness (the synchronization contention, i.e. the time that threads a blocked by others) of the system and the synchronization management overhead. Furthermore it can directly influence the presence of higher-level data dependencies, that
``cannot [be] directly inferred via data dependencies''. To this end, \AE minium offers data groups that allow the user to specify, which data must be synchronized together. An according language feature would enrich ParallelMbeddr with better support for higher-level dependencies. Furthermore, data groups would allow the compiler to make use of less mutexes.

\subsection{D}
A similar feature to data groups in \AE minium is offered by the multi-paradigm language \textit{D}.\footnote{http://dlang.org/, accessed: 2014-08-23} D provides a thread-safety-first approach to parallelization. The programmer has to manually protected data that is shared between threads. One way of achieving this goal is by synchronizing classes. If a class is synchronized its data must be private and every call of one of its methods is locked for unique access. If fields themselves have non-primitive types, i.e. class-types, their classes must also be synchronized. In order to tie the protection of such a field and an object more closely, D allows objects to be owned by others in terms of their mutexes. An owned object uses the mutex of its owner. D thus enables similar semantics to data groups in \AE minium. While this feature is still lacking in ParallelMbeddr, the language offers D's support for explicit synchronization for multiple objects in one synchronization statement for coarse-grained, deadlock-free synchronization. While ParallelMbeddr uses a busy-waiting approach to achieve this goal, D uses mutex ordering, which acquires mutexes in the same order in all threads, regardless of the syntactic order in the programmer's code.\footnote{See http://www.informit.com/articles/article.aspx?p=1609144, accessed: 2014-08-23, for details on D and for general information on this technique, see https://www.securecoding.cert.org/confluence/display/seccode/POS51-C.+Avoid+deadlock+with+POSIX+threads+by+locking+in+predefined+order, accessed: 2014-08-23.}

\subsection{Java}
The Java programming language has a similar form of synchronization as D as. However, its guarantees concerning thread-safety are considerably weaker. In Java, the focus of synchronizations is rather computation-oriented than data-oriented like in D. The programmer is himself responsible of identifying the blocks of statements and methods that need to be synchronized and needs to annotate them. Unlike D, a class can thus have synchronized and not synchronized methods. Furthermore, although the access through a method $m$ of such a monitor (i.e. instance of a class) is synchronized, simultaneous access via unsynchronized other methods to the same data, which $m$ accesses, is still possible. In spite of these limitations, the research on lock-related optimizations in Java was used as a starting point for similar optimizations in ParallelMbeddr. The optimizations accomplished in Java include lock-elision for re-entrant monitors, i.e. monitors whose synchronized methods are called recursively. Furthermore, enclosed and thread-local monitors are optimized. The latter relates to single-task lock elision in ParallelMbeddr. The former can be seen as an optimization technique that was not applied in ParallelMbeddr. If the field $f$ of a monitor $m$ is always accessed by a synchronized method of $m$ and $f$ is itself a monitor, then the synchronizations for $f$'s methods can be omitted. Similar, in ParallelMbeddr the synchronizations for a shared resource $n$, which is nested inside another one $s$, could be omitted, if $n$ is always synchronized inside a synchronization context of $s$ \cite{StaticAnalysesForJava}. Another optimization applied in Java is lock-coarsening, which tries to reduce the synchronization management overhead by broadening synchronization contexts \cite{JavaTheoryAndPractice}. Such optimization can increase the amount of lock contention. For this reason, good heuristics are mandatory. Another optimization is adaptive locking which switches between spinning (for short time intervals) and suspension (for longer ones) when a thread waits for the release of a lock. Another technique, which was proposed for Java, is lock reservation \cite{LockReservation}. Lock reservation assumes that locks are repeatedly requested by one thread at a time. It exploits this property by storing at runtime for each lock an owner-thread which might change anytime. While accesses via a lock $l$ of the $o$ owner thread need not be locked, other threads have to lock $l$. Hence, if for a certain amount of time, $o$ is the only thread to enter the synchronized methods of a monitor, protected by $l$, then meanwhile no locking for $l$ has to be applied. It remains to be investigated, if such optimization is suitable for the resource-constraint embedded domain.

\subsection{Rust}
While Java tries to reduce the amount of necessary locks at compilation time, the programming language \textit{Rust}\footnote{http://www.rust-lang.org/, accessed: 2014-23-08} pushes this effort to the programmer by providing appropriate data types. As a systems programming language, Rust targets performance, but also memory-safety (i.e. null pointers and dangling pointers are prevented) and thread-safety. Among other things, the latter is accomplished by the use of owned and borrowed pointers. If data is owned by a thread, the thread can safely access it without the need of synchronization. The data can be used inside functions by borrowing a pointer to the function. This pointer has a limited lifetime and can never leave its creator thread. However, ownership can be transfered to other threads. For data that need to be accessed from multiple threads, Rust offers the wrapper type \CODE{Mutex<T>} which ``provides synchronized access to the underlying data''.\footnote{http://doc.rust-lang.org/sync/struct.Mutex.html, accessed: 2014-08-23} A mutex in Rust is the equivalent to a shared resource in ParallelMbeddr. Any access to the underlying data must be accompanied by a lock of the mutex container. Rust further offers a signal-wait approach via condition variables for the type \CODE{Mutex<T>}. Concluding, with the depicted language features and various others, Rust offers manifold opportunities to avoid locks in the first place. For this reason, Rust can serve as a prototype for future language-based enhancements of ParallelMbeddr.

- owned, borrowed pointers, prevent data-races
- mutable data locked wrapped inside mutex-protected containers, principle equals shared resources
- has condition variables
- unsafe blocks to avoid synchronization

